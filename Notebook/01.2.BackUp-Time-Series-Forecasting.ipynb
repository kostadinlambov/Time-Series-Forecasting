{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438baa76-8413-4ab6-b3a2-0709d6bf497e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e17c7344-d26d-4be5-b74e-8c227b5f0588",
   "metadata": {},
   "source": [
    "# Time-Series-Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d9e10-79f3-4f3d-a929-84b04e5a7c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a31b4e34-6f06-48fe-af36-445d74b3cafd",
   "metadata": {},
   "source": [
    "## 0. Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd1752-2c0d-433b-b4c2-96efa37a8ec9",
   "metadata": {},
   "source": [
    "## 0.1. Which neural networks could be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c2cda-e0f7-48eb-a535-96cfb488472a",
   "metadata": {},
   "source": [
    "I have a set with Time series data of the bitcoin hourly price for the last  years. The set hat 500000 rows. I want to train a neural network to predict the price in the next hour. I want to use also some technical indicators like RSI, SMA, EMA, MACD. I want to use also Seasonality and Trends, Autocorellations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8adb82-d134-48af-8165-f9732e4e40b1",
   "metadata": {},
   "source": [
    "To predict the next hourly Bitcoin price using your dataset, you can choose from several neural network architectures tailored to time-series forecasting. Here’s an overview of suitable networks and how they can incorporate technical indicators, seasonality, trends, and autocorrelations:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Recurrent Neural Networks (RNNs)**\n",
    "   - **Simple RNN**: Captures sequential dependencies but may struggle with long-term dependencies.\n",
    "   - **Long Short-Term Memory (LSTM)**:\n",
    "     - Designed to address the vanishing gradient problem, making it better for long-term dependencies in time series.\n",
    "     - Use to model autocorrelations and temporal patterns effectively.\n",
    "   - **Gated Recurrent Units (GRU)**:\n",
    "     - Similar to LSTMs but with fewer parameters, often faster to train.\n",
    "\n",
    "   **How to use:** \n",
    "   - Input historical prices and technical indicators as features.\n",
    "   - Augment the input with Fourier-transformed or seasonal components for seasonality.\n",
    "   - Use lagged features for autocorrelations.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Convolutional Neural Networks (CNNs)**\n",
    "   - Surprisingly effective for time-series forecasting by capturing local temporal patterns.\n",
    "   - Use 1D convolutions to extract patterns from sequences of technical indicators and price history.\n",
    "\n",
    "   **How to use:**\n",
    "   - Stack CNN layers followed by fully connected layers.\n",
    "   - Use sliding windows of input data (e.g., the past 24 hours of features).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Temporal Convolutional Networks (TCNs)**\n",
    "   - A modern alternative to RNNs with dilated convolutions to capture long-term dependencies.\n",
    "   - Handles autoregressive tasks efficiently.\n",
    "\n",
    "   **How to use:**\n",
    "   - Use technical indicators and trends as input features.\n",
    "   - Extract temporal dependencies without requiring sequential processing.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Transformer Models**\n",
    "   - Initially designed for NLP but now adapted for time-series forecasting.\n",
    "   - **Time-Series Transformers (TSTs)** are specifically tailored for this purpose.\n",
    "\n",
    "   **How to use:**\n",
    "   - Incorporate positional encoding for temporal information.\n",
    "   - Use self-attention to weigh the importance of past data points, technical indicators, and trends.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Hybrid Architectures**\n",
    "   - Combine models to leverage strengths of each:\n",
    "     - **CNN + LSTM**: Use CNN layers to extract local patterns, followed by LSTMs for temporal dependencies.\n",
    "     - **Attention Mechanisms in LSTMs**: Enhance LSTM predictions by focusing on relevant time steps.\n",
    "     - **Transformer + LSTM/CNN**: Use Transformers for long-term dependencies and CNNs for feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Autoregressive Models with Neural Networks**\n",
    "   - Combine autoregressive (AR) components with deep learning.\n",
    "   - Neural networks model nonlinearities, while AR captures autocorrelation directly.\n",
    "\n",
    "   **How to use:**\n",
    "   - Incorporate lagged prices, technical indicators, and residuals from trend decomposition.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **DeepAR or Probabilistic Models**\n",
    "   - Predict probabilistic distributions instead of point estimates.\n",
    "   - Useful for capturing uncertainty in volatile datasets like Bitcoin prices.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Graph Neural Networks (GNNs)**\n",
    "   - Incorporate relationships between features (e.g., correlations among technical indicators).\n",
    "   - Less common but can add a new perspective.\n",
    "\n",
    "---\n",
    "\n",
    "### Incorporating Seasonality, Trends, and Autocorrelations:\n",
    "1. **Seasonality**:\n",
    "   - Use Fourier transformations or seasonal decomposition (STL) to extract periodic components.\n",
    "   - Include these as additional input features.\n",
    "\n",
    "2. **Trends**:\n",
    "   - Detrend the data and feed both original and detrended values into the model.\n",
    "   - Use techniques like Hodrick-Prescott filter to isolate trends.\n",
    "\n",
    "3. **Autocorrelations**:\n",
    "   - Include lagged features or rolling statistics (e.g., past n-hour averages, autocorrelation lags).\n",
    "\n",
    "---\n",
    "\n",
    "### Tools and Frameworks:\n",
    "- **Keras/TensorFlow**: Flexible and user-friendly.\n",
    "- **PyTorch**: Excellent for custom and advanced models.\n",
    "- **Darts, GluonTS**: Frameworks focused on time-series forecasting.\n",
    "\n",
    "Let me know if you'd like detailed code or a step-by-step guide for any specific architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842e30e-08db-4f60-b5d8-fb956198e0fd",
   "metadata": {},
   "source": [
    "## 02. Hybrid Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8c329-7426-4788-b59a-b0379260c5dc",
   "metadata": {},
   "source": [
    "I want to use a Hybrid Architectures. Give me more information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eebb7-7496-4f62-b7a1-36a7d5302298",
   "metadata": {},
   "source": [
    "Hybrid architectures combine the strengths of multiple neural network types to handle the complex dynamics of time-series data, such as trends, seasonality, and autocorrelations. Here’s how you can design and implement a hybrid architecture for your Bitcoin hourly price prediction:\n",
    "\n",
    "---\n",
    "\n",
    "### **Components of a Hybrid Architecture**\n",
    "\n",
    "### 1. **CNN for Feature Extraction**\n",
    "   - **Purpose**: Captures short-term patterns and local temporal dependencies from the data.\n",
    "   - **Implementation**:\n",
    "     - Use 1D convolutions over the time-series data (e.g., past 24–48 hours).\n",
    "     - Extract features from technical indicators (RSI, SMA, EMA, MACD) and raw prices.\n",
    "\n",
    "   **Key Hyperparameters**:\n",
    "   - Kernel size: Determines how many time steps are analyzed together.\n",
    "   - Strides: Controls the overlap between convolution windows.\n",
    "\n",
    "   **Output**:\n",
    "   - Feature maps summarizing localized patterns in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **LSTM/GRU for Temporal Dependencies**\n",
    "   - **Purpose**: Captures long-term dependencies and sequential trends in the data.\n",
    "   - **Implementation**:\n",
    "     - Feed the feature maps from the CNN into an LSTM or GRU.\n",
    "     - Use the memory state to capture temporal relationships in extracted features.\n",
    "\n",
    "   **Key Hyperparameters**:\n",
    "   - Number of units: Controls the memory capacity of the network.\n",
    "   - Dropout: Prevents overfitting by randomly deactivating neurons.\n",
    "\n",
    "   **Output**:\n",
    "   - Temporal feature representations.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Attention Mechanism**\n",
    "   - **Purpose**: Highlights the most relevant parts of the sequence for the prediction.\n",
    "   - **Implementation**:\n",
    "     - Add an attention layer on top of the LSTM/GRU output.\n",
    "     - Compute a weighted sum of the LSTM outputs, where weights are determined dynamically.\n",
    "\n",
    "   **Key Benefit**:\n",
    "   - Allows the model to focus on important time steps (e.g., recent volatility spikes).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Dense (Fully Connected) Layers**\n",
    "   - **Purpose**: Integrates all extracted features and generates the final prediction.\n",
    "   - **Implementation**:\n",
    "     - Combine outputs from CNN, LSTM, and attention layers.\n",
    "     - Add dense layers with ReLU activations.\n",
    "     - Use a linear activation for the output layer (for price regression).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Additional Components**\n",
    "   - **Seasonality and Trends**:\n",
    "     - Decompose the series using STL or Fourier Transform.\n",
    "     - Add the decomposed components as input features.\n",
    "   - **Lagged Features**:\n",
    "     - Include rolling statistics, such as moving averages or lagged values.\n",
    "   - **External Data**:\n",
    "     - Incorporate macroeconomic indicators, sentiment analysis, or news-derived features if available.\n",
    "\n",
    "---\n",
    "\n",
    "### **Architecture Design**\n",
    "Here’s a step-by-step implementation plan:\n",
    "\n",
    "### Input Data:\n",
    "1. **Raw Data**: Hourly Bitcoin prices.\n",
    "2. **Technical Indicators**: RSI, SMA, EMA, MACD.\n",
    "3. **Additional Features**: Lagged features, seasonality, and trends.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Architecture:\n",
    "```python\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, LSTM, GRU, Dense, Dropout, Flatten, Attention, Concatenate\n",
    ")\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(sequence_length, num_features))\n",
    "\n",
    "# CNN for feature extraction\n",
    "cnn_layer = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
    "cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "cnn_layer = Flatten()(cnn_layer)\n",
    "\n",
    "# LSTM for temporal dependencies\n",
    "lstm_layer = LSTM(64, return_sequences=True)(input_layer)\n",
    "attention_output = Attention()([lstm_layer, lstm_layer])  # Self-attention mechanism\n",
    "lstm_output = Flatten()(attention_output)\n",
    "\n",
    "# Combine CNN and LSTM outputs\n",
    "combined = Concatenate()([cnn_layer, lstm_output])\n",
    "\n",
    "# Fully connected layers\n",
    "dense_layer = Dense(128, activation='relu')(combined)\n",
    "dense_layer = Dropout(0.2)(dense_layer)\n",
    "dense_layer = Dense(64, activation='relu')(dense_layer)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Plan**\n",
    "1. **Data Preprocessing**:\n",
    "   - Normalize the data to a range of [0, 1].\n",
    "   - Create sequences of `n` past hours as input and the next hour’s price as the target.\n",
    "   - Train-test split (e.g., 80%-20%).\n",
    "   - Handle missing data and outliers if present.\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Sequence length: Test with different window sizes (e.g., 24, 48, 72 hours).\n",
    "   - Batch size: Experiment with sizes like 32, 64, 128.\n",
    "   - Optimizer: Use Adam with learning rate scheduling.\n",
    "\n",
    "3. **Evaluation Metrics**:\n",
    "   - Root Mean Squared Error (RMSE).\n",
    "   - Mean Absolute Error (MAE).\n",
    "   - Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "---\n",
    "\n",
    "### **Benefits of Hybrid Architectures**\n",
    "- **CNN**: Efficiently captures short-term fluctuations and relationships between technical indicators.\n",
    "- **LSTM/GRU**: Models temporal dependencies and autocorrelations effectively.\n",
    "- **Attention**: Focuses on critical moments in the sequence.\n",
    "- **Integration of Seasonality/Trends**: Improves long-term forecasting accuracy.\n",
    "\n",
    "This setup is versatile and can adapt to various time-series forecasting challenges, such as volatility and seasonal variations. Let me know if you'd like help with specific implementation details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5a7c0-987b-4d97-aa3c-0dc2ef5751e4",
   "metadata": {},
   "source": [
    "## 03. Preparing of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc07baae-6227-4e57-9d4a-54d70896682f",
   "metadata": {},
   "source": [
    "Give me more details for the preparing of the dataset. My dataset hat the following columns: [timestamp,\topen, high, low,\tclose, volume]. Step by step pls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8f79f-ac69-4d8a-991c-a3f478d68084",
   "metadata": {},
   "source": [
    "Preparing your dataset for a neural network involves several steps, including feature engineering, scaling, and splitting the data into training and testing sets. Here’s a step-by-step guide tailored to your Bitcoin dataset with columns `[timestamp, open, high, low, close, volume]`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Import Libraries**\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Load the Dataset**\n",
    "```python\n",
    "# Load your dataset\n",
    "df = pd.read_csv('bitcoin_data.csv')\n",
    "\n",
    "# Convert the timestamp column to a datetime object\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Sort the data by timestamp (if not already sorted)\n",
    "df = df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Feature Engineering**\n",
    "### 3.1. **Add Technical Indicators**\n",
    "Use libraries like `ta` to calculate technical indicators:\n",
    "```python\n",
    "import ta\n",
    "\n",
    "# Relative Strength Index (RSI)\n",
    "df['RSI'] = ta.momentum.RSIIndicator(close=df['close'], window=14).rsi()\n",
    "\n",
    "# Simple Moving Average (SMA)\n",
    "df['SMA'] = df['close'].rolling(window=14).mean()\n",
    "\n",
    "# Exponential Moving Averages (EMA12, EMA26)\n",
    "df['EMA12'] = ta.trend.EMAIndicator(close=df['close'], window=12).ema_indicator()\n",
    "df['EMA26'] = ta.trend.EMAIndicator(close=df['close'], window=26).ema_indicator()\n",
    "\n",
    "# MACD and Signal Line\n",
    "macd = ta.trend.MACD(close=df['close'])\n",
    "df['MACD'] = macd.macd()\n",
    "df['Signal_Line'] = macd.macd_signal()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2. **Add Lagged Features**\n",
    "Lagged features capture autocorrelation and temporal patterns.\n",
    "```python\n",
    "# Create lagged close prices (e.g., past 3 hours)\n",
    "for lag in range(1, 4):\n",
    "    df[f'lag_{lag}'] = df['close'].shift(lag)\n",
    "\n",
    "# Create lagged volume\n",
    "for lag in range(1, 4):\n",
    "    df[f'volume_lag_{lag}'] = df['volume'].shift(lag)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3. **Add Rolling Statistics**\n",
    "Rolling statistics capture trends and smoothing.\n",
    "```python\n",
    "# Rolling mean and standard deviation of close price\n",
    "df['rolling_mean'] = df['close'].rolling(window=24).mean()  # 24-hour rolling mean\n",
    "df['rolling_std'] = df['close'].rolling(window=24).std()   # 24-hour rolling std\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4. **Add Time-Based Features**\n",
    "Extract seasonal and cyclical patterns from the timestamp.\n",
    "```python\n",
    "# Hour, day of the week, day of the month, etc.\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "# Add cyclical transformations for time-based features\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Create Target Variable**\n",
    "### Predict the **next hour’s price**:\n",
    "```python\n",
    "# Target: Next hour's close price\n",
    "df['target'] = df['close'].shift(-1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Handle Missing Values**\n",
    "Some features (e.g., RSI, lagged values) will have NaN values due to the calculation windows.\n",
    "```python\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Scale the Data**\n",
    "Neural networks perform better when data is scaled. Use `MinMaxScaler` for normalization.\n",
    "```python\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select features for scaling\n",
    "features_to_scale = ['open', 'high', 'low', 'close', 'volume', \n",
    "                     'RSI', 'SMA', 'EMA12', 'EMA26', 'MACD', 'Signal_Line', \n",
    "                     'rolling_mean', 'rolling_std'] + \\\n",
    "                     [f'lag_{lag}' for lag in range(1, 4)] + \\\n",
    "                     [f'volume_lag_{lag}' for lag in range(1, 4)]\n",
    "\n",
    "# Scale features\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Create Sequences**\n",
    "Neural networks like CNNs, LSTMs, and GRUs require sequential input data. Convert your data into overlapping sequences.\n",
    "\n",
    "### Define Sequence Length:\n",
    "- For example, use the past **24 hours** to predict the next hour.\n",
    "```python\n",
    "sequence_length = 24\n",
    "\n",
    "# Prepare sequences\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(df.iloc[i:i + sequence_length][features_to_scale].values)\n",
    "    y.append(df.iloc[i + sequence_length]['target'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Train-Test Split**\n",
    "Split your dataset into training, validation, and testing sets.\n",
    "```python\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Further split the training set into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 9: Prepare for Neural Networks**\n",
    "Ensure the data is ready for your hybrid neural network:\n",
    "- **Input Shape**: `(batch_size, sequence_length, num_features)`\n",
    "- **Output**: Single value (the next hour’s price).\n",
    "\n",
    "```python\n",
    "print(\"X_train shape:\", X_train.shape)  # (samples, sequence_length, num_features)\n",
    "print(\"y_train shape:\", y_train.shape)  # (samples,)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "- Use this preprocessed dataset to train your hybrid architecture.\n",
    "- Tune hyperparameters, monitor training performance using RMSE or MAE, and adjust features as needed.\n",
    "\n",
    "Let me know if you'd like help with a specific model implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bce3ef-49b6-4389-8114-e9a9c6209683",
   "metadata": {},
   "source": [
    "## 04. Training. Hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78eff2-d0cf-4e41-b0ea-d17f36e2d16e",
   "metadata": {},
   "source": [
    "Give an example how to use the data for the training. How can i tune the hyperparameters. Step by step pls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68d5c5-2e17-4880-8c79-ce2086151b88",
   "metadata": {},
   "source": [
    "Here’s a step-by-step guide on how to use your prepared dataset for training a neural network and tuning hyperparameters:\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Import Libraries**\n",
    "Start by importing the necessary libraries for building, training, and evaluating the model.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Input, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Define Model Architecture**\n",
    "Let’s use a hybrid CNN + LSTM architecture for time-series forecasting.\n",
    "\n",
    "```python\n",
    "def build_model(sequence_length, num_features):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(sequence_length, num_features))\n",
    "    \n",
    "    # CNN for feature extraction\n",
    "    cnn_layer = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Flatten()(cnn_layer)\n",
    "    \n",
    "    # LSTM for temporal dependencies\n",
    "    lstm_layer = LSTM(64, return_sequences=False)(input_layer)\n",
    "    \n",
    "    # Combine CNN and LSTM outputs\n",
    "    combined = Concatenate()([cnn_layer, lstm_layer])\n",
    "    \n",
    "    # Fully connected layers\n",
    "    dense_layer = Dense(128, activation='relu')(combined)\n",
    "    dense_layer = Dropout(0.2)(dense_layer)\n",
    "    dense_layer = Dense(64, activation='relu')(dense_layer)\n",
    "    \n",
    "    # Output layer (regression)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Initialize the Model**\n",
    "Build your model by specifying the input shape based on your data.\n",
    "\n",
    "```python\n",
    "sequence_length = X_train.shape[1]  # Length of the sequences (e.g., 24 hours)\n",
    "num_features = X_train.shape[2]  # Number of features in the dataset\n",
    "\n",
    "model = build_model(sequence_length, num_features)\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Train the Model**\n",
    "Set up training parameters and callbacks to monitor performance.\n",
    "\n",
    "### Early Stopping\n",
    "Early stopping prevents overfitting by stopping training when the validation performance stops improving.\n",
    "\n",
    "```python\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "```\n",
    "\n",
    "### Training\n",
    "Train the model with your training data, using validation data to track performance.\n",
    "\n",
    "```python\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  # You can adjust this\n",
    "    batch_size=32,  # Experiment with batch sizes\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Evaluate the Model**\n",
    "Evaluate the model on the test set to see how well it generalizes.\n",
    "\n",
    "```python\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Hyperparameter Tuning**\n",
    "To tune hyperparameters, you can use tools like **Keras Tuner** or manually adjust parameters in a structured way.\n",
    "\n",
    "### Option 1: Grid Search (Manual)\n",
    "Manually try different combinations of hyperparameters:\n",
    "- **Number of filters in CNN (e.g., 16, 32, 64).**\n",
    "- **LSTM units (e.g., 32, 64, 128).**\n",
    "- **Batch size (e.g., 16, 32, 64).**\n",
    "- **Learning rate (e.g., 0.001, 0.0001).**\n",
    "- **Dropout rate (e.g., 0.2, 0.3, 0.4).**\n",
    "\n",
    "Example:\n",
    "```python\n",
    "for filters in [16, 32, 64]:\n",
    "    for lstm_units in [32, 64, 128]:\n",
    "        model = build_model(sequence_length, num_features)\n",
    "        print(f\"Training with filters={filters}, lstm_units={lstm_units}\")\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Option 2: Keras Tuner (Automated)\n",
    "Use **Keras Tuner** to automate hyperparameter tuning.\n",
    "\n",
    "```python\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Define a model-building function for Keras Tuner\n",
    "def build_tuned_model(hp):\n",
    "    input_layer = Input(shape=(sequence_length, num_features))\n",
    "    \n",
    "    # CNN\n",
    "    cnn_layer = Conv1D(\n",
    "        filters=hp.Int('filters', min_value=16, max_value=64, step=16), \n",
    "        kernel_size=3, activation='relu'\n",
    "    )(input_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Flatten()(cnn_layer)\n",
    "    \n",
    "    # LSTM\n",
    "    lstm_layer = LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=32, max_value=128, step=32), \n",
    "        return_sequences=False\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Combine\n",
    "    combined = Concatenate()([cnn_layer, lstm_layer])\n",
    "    dense_layer = Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=256, step=64), \n",
    "        activation='relu'\n",
    "    )(combined)\n",
    "    dense_layer = Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1))(dense_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [0.001, 0.0005, 0.0001])),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_tuned_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_tuning_dir',\n",
    "    project_name='bitcoin_forecasting'\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "\n",
    "# Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Save and Load the Model**\n",
    "Save the best model for future use.\n",
    "\n",
    "```python\n",
    "# Save model\n",
    "model.save('bitcoin_price_model.h5')\n",
    "\n",
    "# Load model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('bitcoin_price_model.h5')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Forecast Future Prices**\n",
    "Once the model is trained, you can use it to forecast future prices.\n",
    "\n",
    "```python\n",
    "# Prepare a sequence of the last 24 hours\n",
    "latest_sequence = X_test[-1].reshape(1, sequence_length, num_features)\n",
    "\n",
    "# Predict the next hour's price\n",
    "next_price = model.predict(latest_sequence)\n",
    "print(f\"Predicted next price: {next_price[0][0]}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This approach allows you to train, evaluate, and tune your model effectively. Let me know if you need more help!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a565add-a3b2-45bc-b685-61c273765db9",
   "metadata": {},
   "source": [
    "## 05. Hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f6886e-7901-4b95-ad64-fb3a509ee7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d460c0d-6d9c-49ff-8270-54f956433ae3",
   "metadata": {},
   "source": [
    "Here's a step-by-step guide to hyperparameter tuning with **Optuna**, a flexible and efficient framework for automated hyperparameter optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Install Optuna**\n",
    "First, ensure you have Optuna installed. If not, install it using:\n",
    "\n",
    "```bash\n",
    "pip install optuna\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Import Libraries**\n",
    "Import the required libraries, including Optuna.\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Input, Concatenate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Define the Objective Function**\n",
    "The objective function defines the model architecture, hyperparameters, and training process. Optuna will optimize this function by finding the combination of hyperparameters that minimizes the validation loss.\n",
    "\n",
    "```python\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    filters = trial.suggest_int('filters', 16, 64, step=16)  # Filters for CNN\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 128, step=32)  # Units in LSTM\n",
    "    dense_units = trial.suggest_int('dense_units', 64, 256, step=64)  # Units in Dense layer\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5, step=0.1)  # Dropout rate\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)  # Learning rate\n",
    "\n",
    "    # Build the model\n",
    "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    # CNN\n",
    "    cnn_layer = Conv1D(filters=filters, kernel_size=3, activation='relu')(input_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Flatten()(cnn_layer)\n",
    "\n",
    "    # LSTM\n",
    "    lstm_layer = LSTM(lstm_units, return_sequences=False)(input_layer)\n",
    "\n",
    "    # Combine CNN and LSTM\n",
    "    combined = Concatenate()([cnn_layer, lstm_layer])\n",
    "\n",
    "    # Dense layers\n",
    "    dense_layer = Dense(dense_units, activation='relu')(combined)\n",
    "    dense_layer = Dropout(dropout_rate)(dense_layer)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=10,  # Keep epochs small for quicker tuning\n",
    "        batch_size=32,  # You can also tune this as a hyperparameter\n",
    "        verbose=0  # Suppress output during training\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return val_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Create an Optuna Study**\n",
    "Define a study and specify the optimization direction (`minimize` for validation loss).\n",
    "\n",
    "```python\n",
    "study = optuna.create_study(direction='minimize')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Run the Optimization**\n",
    "Run the optimization for a specified number of trials.\n",
    "\n",
    "```python\n",
    "study.optimize(objective, n_trials=20)  # Adjust the number of trials as needed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Get the Best Hyperparameters**\n",
    "After the optimization completes, retrieve the best hyperparameters and validation loss.\n",
    "\n",
    "```python\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "print(\"Best Validation Loss:\", study.best_value)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Train the Final Model with Best Hyperparameters**\n",
    "Rebuild and train the model using the best hyperparameters.\n",
    "\n",
    "```python\n",
    "best_params = study.best_params\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "# CNN\n",
    "cnn_layer = Conv1D(filters=best_params['filters'], kernel_size=3, activation='relu')(input_layer)\n",
    "cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "cnn_layer = Flatten()(cnn_layer)\n",
    "\n",
    "# LSTM\n",
    "lstm_layer = LSTM(best_params['lstm_units'], return_sequences=False)(input_layer)\n",
    "\n",
    "# Combine CNN and LSTM\n",
    "combined = Concatenate()([cnn_layer, lstm_layer])\n",
    "\n",
    "# Dense layers\n",
    "dense_layer = Dense(best_params['dense_units'], activation='relu')(combined)\n",
    "dense_layer = Dropout(best_params['dropout_rate'])(dense_layer)\n",
    "output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "\n",
    "# Compile the model\n",
    "final_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "final_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,  # Longer training for final model\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Evaluate the Final Model**\n",
    "Evaluate the model on the test set.\n",
    "\n",
    "```python\n",
    "# Predictions on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 9: Visualize the Results**\n",
    "Visualize the optimization process and loss history.\n",
    "\n",
    "### Visualize Hyperparameter Optimization\n",
    "```python\n",
    "optuna.visualization.plot_optimization_history(study).show()\n",
    "```\n",
    "\n",
    "### Visualize Hyperparameter Importance\n",
    "```python\n",
    "optuna.visualization.plot_param_importances(study).show()\n",
    "```\n",
    "\n",
    "### Training Loss History\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This setup ensures that you efficiently search for the best hyperparameters while maintaining flexibility. Let me know if you need help fine-tuning further!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92657e38-8ccb-49ff-b0d3-b9f1a7241630",
   "metadata": {},
   "source": [
    "## 06. Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c6202-11d8-4ece-881b-8c016a2e068c",
   "metadata": {},
   "source": [
    "Fine-tuning in the context of neural networks involves refining the model to achieve the best performance on your dataset. Below is a detailed guide on different fine-tuning methods you can apply:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Hyperparameter Fine-Tuning**\n",
    "This involves optimizing key parameters such as learning rate, batch size, number of layers, and dropout rates.\n",
    "\n",
    "### Key Parameters to Fine-Tune:\n",
    "- **Learning Rate**: The most crucial hyperparameter. Start with a default value (e.g., 0.001) and adjust using:\n",
    "  - A smaller learning rate (e.g., 0.0001) for fine adjustments.\n",
    "  - A larger learning rate (e.g., 0.01) for faster convergence but with a risk of instability.\n",
    "- **Batch Size**:\n",
    "  - Smaller batch sizes (16, 32) can generalize better but take longer to train.\n",
    "  - Larger batch sizes (64, 128) speed up training but might overfit.\n",
    "- **Dropout Rate**: Prevents overfitting; typically ranges between 0.2 and 0.5.\n",
    "- **Number of Layers and Neurons**:\n",
    "  - Experiment with deeper models or additional neurons in fully connected layers.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Implement Learning Rate Schedules**\n",
    "Dynamic adjustment of the learning rate can enhance training efficiency. Use learning rate schedulers in Keras:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Reduce learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Regularization Fine-Tuning**\n",
    "Regularization helps reduce overfitting and improve model generalization.\n",
    "\n",
    "### Techniques:\n",
    "1. **L2 Regularization (Weight Decay)**:\n",
    "   Penalizes large weights in the loss function.\n",
    "   ```python\n",
    "   from tensorflow.keras.regularizers import l2\n",
    "\n",
    "   dense_layer = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "   ```\n",
    "\n",
    "2. **Dropout**:\n",
    "   Dropout randomly disables neurons during training.\n",
    "   ```python\n",
    "   dropout_layer = Dropout(0.3)(dense_layer)\n",
    "   ```\n",
    "\n",
    "3. **Batch Normalization**:\n",
    "   Speeds up training and stabilizes the model.\n",
    "   ```python\n",
    "   from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "   batch_norm_layer = BatchNormalization()(dense_layer)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Data Augmentation for Time-Series**\n",
    "Introduce variability in training data to improve robustness.\n",
    "\n",
    "### Example Techniques:\n",
    "- **Add Gaussian Noise**:\n",
    "   Slightly alter the training data by adding random noise.\n",
    "   ```python\n",
    "   noise_factor = 0.01\n",
    "   X_train_noisy = X_train + noise_factor * np.random.normal(size=X_train.shape)\n",
    "   ```\n",
    "- **Time Shifting**:\n",
    "   Shift the entire time-series slightly forward or backward.\n",
    "- **Scaling and Normalization Variants**:\n",
    "   Experiment with scaling techniques like Min-Max, Standard Scaling, or Robust Scaling.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Use Transfer Learning**\n",
    "If you have a small dataset, leverage a pre-trained model. While transfer learning is more common in image or NLP tasks, you can use pre-trained time-series models or design a custom transfer learning setup:\n",
    "- **Train a base model on a larger dataset (e.g., S&P500, other crypto data).**\n",
    "- **Freeze early layers** and fine-tune later layers on your dataset.\n",
    "\n",
    "```python\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new trainable layers\n",
    "x = Dense(64, activation='relu')(base_model.output)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(1, activation='linear')(x)\n",
    "\n",
    "# Compile the fine-tuned model\n",
    "fine_tuned_model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "fine_tuned_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Early Stopping**\n",
    "Monitor validation performance and stop training when improvement stalls.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Cross-Validation for Time-Series**\n",
    "Standard k-fold cross-validation can disrupt temporal order in time-series. Use **TimeSeriesSplit** to preserve temporal structure.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, val_index in tscv.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Train your model on this split\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Optimize Training with Mixed Precision**\n",
    "Leverage GPUs by using mixed precision for faster training and reduced memory usage.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "# Build and train the model as usual\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Hyperparameter Optimization with Optuna**\n",
    "If you haven’t already, use Optuna for automated fine-tuning.\n",
    "\n",
    "### Example Parameters to Optimize:\n",
    "- Number of filters in CNN\n",
    "- LSTM units\n",
    "- Dense units\n",
    "- Dropout rate\n",
    "- Learning rate\n",
    "- Batch size\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    filters = trial.suggest_int('filters', 16, 64, step=16)\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 128, step=32)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(filters, lstm_units, dropout_rate, learning_rate)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Return validation loss\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(study.best_params)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Ensemble Models**\n",
    "Combine multiple models (e.g., CNN, LSTM, and Transformer models) to improve robustness. Train each model separately and average their predictions.\n",
    "\n",
    "```python\n",
    "# Average predictions from multiple models\n",
    "y_pred_ensemble = (y_pred_model1 + y_pred_model2 + y_pred_model3) / 3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Monitoring Training**\n",
    "Use tools like **TensorBoard** to visualize metrics and fine-tune based on trends.\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir logs\n",
    "```\n",
    "\n",
    "Add a callback to log training:\n",
    "\n",
    "```python\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "By applying these fine-tuning techniques systematically, you can significantly improve your model’s performance. Let me know which aspect you'd like more details on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233a796-0cb8-46d1-8e56-0c071edb01fd",
   "metadata": {},
   "source": [
    "## 07. Are 500_000 values enough for the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a76892-6f38-4681-9e3a-d382673a1059",
   "metadata": {},
   "source": [
    "Whether 500,000 values are enough for training your model depends on several factors:\n",
    "\n",
    "---\n",
    "\n",
    "### **Factors to Consider**\n",
    "\n",
    "#### **1. Complexity of the Model**\n",
    "- **Simple models (e.g., MLP, basic LSTM)**: 500,000 rows can be sufficient, especially if the dataset is representative of the problem you're solving.\n",
    "- **Complex models (e.g., hybrid CNN-LSTM, Transformers)**: These require more data to learn patterns effectively and avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Feature Engineering**\n",
    "If you've added meaningful features like technical indicators (RSI, SMA, EMA, MACD) and seasonal/trend components, you reduce the model's reliance on raw data size. Good feature engineering can make 500,000 rows more effective.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Time Span of the Data**\n",
    "- If your data covers **multiple years**, it is more likely to capture different market conditions (bull markets, bear markets, high/low volatility).\n",
    "- If it covers only a short period (e.g., a few months), it might not be diverse enough for a robust model.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Pretraining for Financial Models**\n",
    "Pretraining can improve your model if:\n",
    "- Your dataset isn't large enough.\n",
    "- You want your model to generalize better across different market conditions.\n",
    "- You have access to additional financial data (e.g., other cryptocurrencies, stock prices).\n",
    "\n",
    "For pretraining:\n",
    "1. **Use related datasets**: Pretrain on other cryptocurrencies (e.g., Ethereum, Litecoin) or financial indices (e.g., S&P 500) that share similar patterns.\n",
    "2. **Transfer learning**: Train the model on the additional dataset, freeze the earlier layers, and fine-tune on your Bitcoin dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Cross-Validation**\n",
    "Even with 500,000 rows, it's important to assess the model’s generalization. Use **time-series cross-validation** to ensure your model performs well across different time periods.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Compare with Other Studies**\n",
    "In financial modeling:\n",
    "- Some studies achieve good results with **10,000–100,000 rows** and proper feature engineering.\n",
    "- Larger datasets (1M+ rows) are ideal for training advanced models like Transformers.\n",
    "\n",
    "500,000 rows are **on the larger side for time-series models** and should generally be enough if the dataset is well-preprocessed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Recommendations**\n",
    "1. **Try Training Without Pretraining First**\n",
    "   - With 500,000 rows, you likely don't need pretraining initially.\n",
    "   - Focus on proper preprocessing, feature engineering, and hyperparameter tuning.\n",
    "\n",
    "2. **Augment the Dataset (If Needed)**\n",
    "   - Include more data from other cryptocurrencies or financial instruments to increase diversity.\n",
    "   - Add synthetic data using techniques like time-series augmentation.\n",
    "\n",
    "3. **Monitor Generalization**\n",
    "   - Ensure the model performs well on validation/test sets using metrics like MSE, RMSE, or MAE.\n",
    "   - Test it across different market conditions (e.g., bull vs. bear markets).\n",
    "\n",
    "4. **Experiment with Pretraining**\n",
    "   - If your model overfits or underperforms, pretrain it on a larger dataset (e.g., other crypto or stock market data).\n",
    "   - Use transfer learning to fine-tune it on your specific Bitcoin dataset.\n",
    "\n",
    "---\n",
    "\n",
    "In conclusion, **500,000 values should be sufficient for training your model**, especially if combined with good feature engineering and proper validation. However, if your model struggles to generalize or you want to further boost performance, pretraining on additional data can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01ebbb-655b-40f5-9668-b0aa1966258f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
