{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a98934-7668-4874-9803-ca56626cb38c",
   "metadata": {},
   "source": [
    "# Informer: Beyond Efficient Transformer for Long Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16797d5d-537a-4f16-b899-74769d274391",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Long Sequence Time-Series Forecasting (LSTF) plays a crucial role in various domains, including energy management, finance, and health monitoring. Predicting long time-series sequences requires models capable of capturing long-range dependencies while maintaining computational efficiency. Traditional forecasting models, such as Recurrent Neural Networks (RNNs) and vanilla Transformers, struggle with scalability due to their high computational complexity and memory consumption. To address these limitations, the **Informer** model introduces three primary innovations:\n",
    "\n",
    "1. **ProbSparse Self-Attention** – Reducing the quadratic complexity of vanilla Transformers to O(L log L) by attending only to the most relevant key-query pairs.\n",
    "2. **Self-Attention Distilling** – A technique that compresses redundant information, allowing for improved memory efficiency and better long-range dependency modeling.\n",
    "3. **Generative Style Decoding** – Instead of sequentially generating outputs, the Informer predicts entire sequences in a single forward pass, significantly improving inference speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001664f8-d0d9-4d28-be83-37ad6739e011",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627c549-db90-4d70-99e8-edc0a65943f0",
   "metadata": {},
   "source": [
    "### 2.1. Efficient Self-Attention Mechanism\n",
    "\n",
    "Vanilla Transformers suffer from **O(L²) complexity** due to their full self-attention computation. The Informer introduces **ProbSparse Self-Attention**, which selectively attends to a small subset of key-query pairs based on importance, reducing computational demands without sacrificing accuracy.\n",
    "\n",
    "The standard self-attention mechanism is computed as:\n",
    "\n",
    "$$A(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) V$$\n",
    "\n",
    "where $Q, K, V$ are the query, key, and value matrices, and $d$ is the feature dimension. Informer optimizes this by attending only to the **top-u** most informative key-query interactions, drastically reducing computation:\n",
    "\n",
    "$$A(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T_{\\text{top-u}}}{\\sqrt{d}} \\right) V_{\\text{top-u}}$$\n",
    "\n",
    "This sparsity-aware mechanism enables efficient dependency alignment while preserving predictive quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e2938-232d-43eb-a674-7a21095cbcc1",
   "metadata": {},
   "source": [
    "### 2.2. Self-Attention Distilling\n",
    "\n",
    "To further enhance efficiency, Informer employs a hierarchical approach that **progressively distills attention layers**:\n",
    "\n",
    "- Redundant attention maps are pruned, focusing computation on dominant features.\n",
    "- By applying **max-pooling operations**, attention maps shrink layer by layer, reducing memory consumption from **O(L²) to O((2-ε)L log L)**.\n",
    "\n",
    "The downsampling in distillation is computed as:\n",
    "\n",
    "$$X_{j+1} = \\text{MaxPool} \\left( \\text{ELU} ( \\text{Conv1D}(X_j) ) \\right),$$\n",
    "\n",
    "where $X_j$ is the feature map at layer $j$, $\\text{Conv1D}$ is a one-dimensional convolutional filter, and $\\text{ELU}$ is the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68164a-2ee6-4f07-96ab-5c1e7b1da9bf",
   "metadata": {},
   "source": [
    "### 2.3. Generative Style Decoding\n",
    "\n",
    "Traditional Transformers rely on **dynamic decoding**, which sequentially generates outputs, leading to slow inference speeds and cumulative error propagation. Informer replaces this with a **single-step generative decoder**, which predicts entire sequences at once. The decoder input is given as:\n",
    "\n",
    "$$X_{\\text{de}} = \\text{Concat}(X_{\\text{token}}, X_0)$$\n",
    "\n",
    "where \\(X_{\\text{token}}\\) is a known segment of the sequence and \\(X_0\\) is a zero-padded placeholder for the target values. The final output is computed via a fully connected transformation:\n",
    "\n",
    "$$Y = \\text{FCN}(X_{\\text{de}})$$\n",
    "\n",
    "This non-autoregressive approach eliminates dependence on previous predictions, ensuring robustness against error accumulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7ab09-e6df-47a6-b1ba-c77db899028b",
   "metadata": {},
   "source": [
    "## 3. Experimental Results\n",
    "\n",
    "The Informer model is evaluated on four real-world datasets:\n",
    "\n",
    "- **ETTh1, ETTh2, ETTm1** – Electricity Transformer Temperature datasets containing energy consumption and transformer temperature data.\n",
    "- **ECL (Electricity Consumption Load)** – A dataset tracking hourly electricity consumption for 321 clients.\n",
    "- **Weather** – Climate-based dataset with hourly weather conditions across multiple locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77817511-dbb7-400e-a192-2822875a37ae",
   "metadata": {},
   "source": [
    "### 4. Key Findings\n",
    "\n",
    "1. **Computation Efficiency**\n",
    "\n",
    "   - Informer significantly reduces **training and inference times** due to optimized self-attention and decoding mechanisms.\n",
    "   - It scales well, handling **sequences 10× longer** than traditional Transformer models.\n",
    "\n",
    "2. **Prediction Accuracy**\n",
    "\n",
    "   - Informer consistently achieves lower **Mean Squared Error (MSE)** and **Mean Absolute Error (MAE)** compared to LSTM, ARIMA, Prophet, DeepAR, Reformer, and LogTrans.\n",
    "   - The model maintains high performance across varying forecast lengths, demonstrating **robust long-term predictive power**.\n",
    "\n",
    "3. **Scalability & Memory Usage**\n",
    "\n",
    "   - Compared to standard Transformers, Informer’s **self-attention distilling** and **ProbSparse mechanism** drastically cut memory requirements.\n",
    "   - It enables real-world deployment in large-scale forecasting applications where traditional models fail due to resource constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d2dab-2dff-48de-b817-59baa087cc17",
   "metadata": {},
   "source": [
    "## 5. Practical Applications\n",
    "\n",
    "Informer’s efficiency and scalability make it ideal for multiple real-world applications, including:\n",
    "\n",
    "- **Smart Grid Energy Forecasting** – Predicting electricity demand to optimize power distribution.\n",
    "- **Financial Market Prediction** – Forecasting stock prices and economic trends for investment strategies.\n",
    "- **Climate and Environmental Modeling** – Long-term weather pattern forecasting for disaster prevention and resource planning.\n",
    "- **Healthcare Time-Series Analysis** – Predicting patient vitals and disease progression over extended periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555136f-8814-4fad-b424-c80613983f97",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Informer represents a significant advancement in **long-sequence time-series forecasting**, overcoming the efficiency bottlenecks of traditional Transformer models. By leveraging **ProbSparse Self-Attention**, **Self-Attention Distilling**, and **Generative Decoding**, it achieves superior predictive accuracy while reducing complexity and memory usage. Extensive experiments confirm its scalability and effectiveness across multiple real-world datasets, making Informer a groundbreaking solution for large-scale time-series forecasting challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dcc719-4c01-46e6-acab-16992241c672",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15a084-90d8-42f8-8250-72349801be5e",
   "metadata": {},
   "source": [
    "[1] Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., & Zhang, W. (2020, December 14). Informer: Beyond efficient transformer for long sequence Time-Series forecasting. arXiv.org. https://arxiv.org/abs/2012.07436"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
