{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910f259c-df25-4f10-95d5-ab28c24df643",
   "metadata": {},
   "source": [
    "# Normalization Techniques in Training DNNs (Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24d33d-bfd6-4efb-b4ad-2e2f9e159123",
   "metadata": {},
   "source": [
    "The paper discusses normalization techniques in deep neural networks (DNNs), including their methodology, analysis, and applications. It also covers their role in the Transformer architecture, particularly Layer Normalization (LN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1e2b5-1aef-4e12-955d-1d8e5e2db6b9",
   "metadata": {},
   "source": [
    "## 1. Introduction to Normalization in DNNs\n",
    "Deep neural networks (DNNs) are powerful models for various domains, including computer vision (CV) and natural language processing (NLP). However, training DNNs is often challenging due to issues such as vanishing gradients and unstable optimization landscapes. Normalization techniques mitigate these challenges by standardizing activations and gradients during training.\n",
    "\n",
    "One of the most significant breakthroughs in normalization was **Batch Normalization (BN)**, introduced by Ioffe and Szegedy. BN standardizes layer activations across a mini-batch, stabilizing training and enabling faster convergence. Since then, various other normalization techniques have emerged, each with its unique benefits and trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625b7cc-7737-454c-be86-e69033be5105",
   "metadata": {},
   "source": [
    "## 2. Taxonomy of Normalization Techniques\n",
    "Normalization methods in DNNs can be broadly categorized into:\n",
    "1. **Activation Normalization** - Normalizes activations across different dimensions.\n",
    "2. **Weight Normalization** - Reparameterizes weight matrices to improve optimization.\n",
    "3. **Gradient Normalization** - Stabilizes gradient updates to prevent explosion or vanishing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc072e8e-876f-4ae0-be00-f111df6bec61",
   "metadata": {},
   "source": [
    "### 2.1. Activation Normalization\n",
    "This method normalizes activations to ensure a stable distribution. The most common techniques include:\n",
    "- **Batch Normalization (BN)**:\n",
    "\n",
    "  $$\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "  \n",
    "  where \\(\\mu\\) and \\(\\sigma^2\\) are the mean and variance computed across a batch.\n",
    "\n",
    "- **Layer Normalization (LN)** (used in Transformers):\n",
    "\n",
    "$$\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_L}{\\sqrt{\\sigma_L^2 + \\epsilon}}$$\n",
    "\n",
    "  where normalization is performed across the features instead of the batch.\n",
    "\n",
    "- **Instance Normalization (IN)**: Similar to BN but applied per instance.\n",
    "- **Group Normalization (GN)**: Groups features into subgroups and normalizes within each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ecf5b-50d8-40a4-a346-1a0f2b98f37f",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. Weight Normalization\n",
    "Weight Normalization (WN) reparameterizes weights as:\n",
    "\n",
    "$$w = \\frac{g}{\\|v\\|} v$$\n",
    "\n",
    "where $g$ is a learnable scaling factor and $v$ is the original weight vector. WN helps decouple weight scaling from direction, improving convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d129e8-e0f6-418d-be58-70f974f4de1e",
   "metadata": {},
   "source": [
    "### 2.3. Gradient Normalization\n",
    "Gradient Normalization prevents instability in deep networks. The update rule is:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\frac{\\nabla L}{\\|\\nabla L\\|}$$\n",
    "\n",
    "where $\\|\\nabla L\\|$ is the gradient norm, ensuring controlled updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21848c-2909-46a5-be45-2983ad324a26",
   "metadata": {},
   "source": [
    "## 3. Role of Normalization in Transformers\n",
    "Transformers rely on **Layer Normalization (LN)** instead of Batch Normalization due to variable-length sequences in NLP tasks. LN standardizes inputs across the feature dimension, making it more stable for attention-based architectures.\n",
    "\n",
    "The **self-attention mechanism** in Transformers is computed as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax} \\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "where $Q, K, V$ are query, key, and value matrices. LN ensures stable training by preventing gradient explosion in attention layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f7c24-38e0-4868-bb84-3f3dbfe79f4a",
   "metadata": {},
   "source": [
    "## 4. Theoretical Analysis of Normalization\n",
    "Normalization techniques improve optimization by:\n",
    "1. **Reducing Internal Covariate Shift**: Ensuring stable distributions across layers.\n",
    "2. **Improving Gradient Flow**: Preventing vanishing or exploding gradients.\n",
    "3. **Accelerating Convergence**: Allowing larger learning rates.\n",
    "\n",
    "A key finding is that **scale-invariance** helps optimization by making gradients more predictable:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial \\hat{x}} \\cdot \\frac{\\partial \\hat{x}}{\\partial \\theta}$$\n",
    "where $\\hat{x}$ is the normalized activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1ab98-a608-475e-8a7f-59e8f1d37abb",
   "metadata": {},
   "source": [
    "## 5. Applications of Normalization\n",
    "Normalization is widely used in:\n",
    "- **GANs**: Spectral Normalization stabilizes training by controlling weight magnitudes.\n",
    "- **Reinforcement Learning**: Normalization improves policy gradient estimation.\n",
    "- **Style Transfer**: Instance Normalization enhances feature disentanglement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee8e43-fd15-4286-84ff-2e8d81a8cf24",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Normalization plays a crucial role in training deep networks. While BN remains the most popular, LN is essential for Transformers, and alternative methods like GN and WN provide flexibility. Future research aims to unify these techniques for improved efficiency and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23468734-0c48-48c9-92db-8bfec5fc7212",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821c262-5478-4d8f-b4fa-5f187e00a827",
   "metadata": {},
   "source": [
    "[1] Huang, L., Qin, J., Zhou, Y., Zhu, F., Liu, L., & Shao, L. (2020, September 27). Normalization Techniques in Training DNNs: Methodology, analysis and application. arXiv.org. https://arxiv.org/abs/2009.12836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802181d-7854-455b-a465-59d2a66a0bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
